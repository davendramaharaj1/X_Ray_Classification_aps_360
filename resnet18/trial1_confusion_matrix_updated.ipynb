{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"trial1_confusion_matrix_updated.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMIlE2GSal8PjzKGLpsqRaX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7cce0a65b41f4777ba60a2d1eb5af882":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e141c2f232114b78ba7ecac585fb03e3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1b852e4b0ba54bc3b9627aad8ea8ed46","IPY_MODEL_3721a416ed33435ebe994fcb5a64e3e1"]}},"e141c2f232114b78ba7ecac585fb03e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1b852e4b0ba54bc3b9627aad8ea8ed46":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c328b6b910b24285b23665ba8379751a","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":102502400,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":102502400,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_00e75dbcbcb0475ebaaf77aa532d919d"}},"3721a416ed33435ebe994fcb5a64e3e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ac65e7aec00943a39d4dc9512b471a05","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 97.8M/97.8M [00:01&lt;00:00, 90.5MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2cdb3706fa14468d933e16a14612e369"}},"c328b6b910b24285b23665ba8379751a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"00e75dbcbcb0475ebaaf77aa532d919d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ac65e7aec00943a39d4dc9512b471a05":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2cdb3706fa14468d933e16a14612e369":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"kzbvZEz0k78X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617315439563,"user_tz":240,"elapsed":4814,"user":{"displayName":"APS360 GroupProj","photoUrl":"","userId":"01981303678726418317"}},"outputId":"931e5a7f-85c1-4098-d653-3127144d799b"},"source":["import os\n","import shutil\n","import random\n","import numpy as np\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import torchvision.models\n","from PIL import Image\n","\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import confusion_matrix\n","from sklearn.externals import joblib\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import plot_confusion_matrix\n","from sklearn.metrics import *\n","import seaborn"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nxeaMC5XlWXb","executionInfo":{"status":"ok","timestamp":1617315439568,"user_tz":240,"elapsed":4797,"user":{"displayName":"APS360 GroupProj","photoUrl":"","userId":"01981303678726418317"}},"outputId":"c459afa3-20f2-4583-99f8-f0d93622a971"},"source":["\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s3R490mJoQ2C","executionInfo":{"status":"ok","timestamp":1617315439572,"user_tz":240,"elapsed":4790,"user":{"displayName":"APS360 GroupProj","photoUrl":"","userId":"01981303678726418317"}}},"source":["dataset_main_path = '/content/gdrive/MyDrive/AllData/Xray_Dataset/'"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"-qk0rCOll_Fz","executionInfo":{"status":"ok","timestamp":1617315439576,"user_tz":240,"elapsed":4786,"user":{"displayName":"APS360 GroupProj","photoUrl":"","userId":"01981303678726418317"}}},"source":["def load_aug_data(batch_size=64):\n","    # Define a transform function that resizes images to 224x224\n","    transform = torchvision.transforms.Compose([torchvision.transforms.Resize((224, 224)), \n","                                                  torchvision.transforms.ToTensor()])\n","\n","    # Load training, validation and test data \n","    train_data = torchvision.datasets.ImageFolder(dataset_main_path+'Train', transform=transform)\n","    val_data = torchvision.datasets.ImageFolder(dataset_main_path+'Validation', transform=transform)\n","    test_data = torchvision.datasets.ImageFolder(dataset_main_path+'Test', transform=transform)\n","\n","    # Training Data is augmented using three techniques\n","    aug_types = [torchvision.transforms.RandomRotation(random.randint(0,10)),                  \n","                torchvision.transforms.RandomAffine(degrees=0, translate=(0.02, 0.02)), \n","                torchvision.transforms.RandomHorizontalFlip(1)]\n","\n","    # Create augmented training data\n","    end_index_1 = 512\n","    end_index_2 = 1024\n","    end_index_3 = 1536\n","\n","    train_indices = [list(range(0, end_index_1)), \n","                    list(range(end_index_1, end_index_2)), \n","                    list(range(end_index_2, end_index_3))]\n","\n","    \n","    transform = torchvision.transforms.Compose([aug_types[0],\n","                                                    torchvision.transforms.Resize((224, 224)),\n","                                                    torchvision.transforms.ToTensor()])\n","    aug_dataset_1 = torchvision.datasets.ImageFolder(dataset_main_path+'Train', transform=transform)\n","    aug_dataset_subset_1 = torch.utils.data.Subset(aug_dataset_1, train_indices[0])\n","    #train_data_new_1 = torch.utils.data.ConcatDataset([train_data, aug_dataset_subset_1])\n","\n","    transform = torchvision.transforms.Compose([aug_types[1],\n","                                                    torchvision.transforms.Resize((224, 224)),\n","                                                    torchvision.transforms.ToTensor()])\n","    aug_dataset_2 = torchvision.datasets.ImageFolder(dataset_main_path+'Train', transform=transform)\n","    aug_dataset_subset_2 = torch.utils.data.Subset(aug_dataset_2, train_indices[1])\n","    #train_data_new_2 = torch.utils.data.ConcatDataset([train_data, aug_dataset_subset])\n","\n","    transform = torchvision.transforms.Compose([aug_types[2],\n","                                                    torchvision.transforms.Resize((224, 224)),\n","                                                    torchvision.transforms.ToTensor()])\n","    aug_dataset_3 = torchvision.datasets.ImageFolder(dataset_main_path+'Train', transform=transform)\n","    aug_dataset_subset_3 = torch.utils.data.Subset(aug_dataset_3, train_indices[2])\n","    train_data_new = torch.utils.data.ConcatDataset([train_data, \n","                                                     aug_dataset_subset_1, \n","                                                     aug_dataset_subset_2, \n","                                                     aug_dataset_subset_3])\n","    \n","    print('Training data:', len(train_data))\n","    print('Training Augmented data:', len(train_data_new))\n","    print('Validation data:',len(val_data))\n","    print('Testing data:',len(test_data))\n","\n","    # The loaders with the augmented data\n","    train_loader = torch.utils.data.DataLoader(train_data_new, batch_size=batch_size, shuffle=True)\n","    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n","    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n","\n","    return train_loader, val_loader, test_loader\n","\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dFX9ggYVomOA"},"source":["### ResNet50 "]},{"cell_type":"code","metadata":{"id":"A9n_gcMFl0L7","colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["7cce0a65b41f4777ba60a2d1eb5af882","e141c2f232114b78ba7ecac585fb03e3","1b852e4b0ba54bc3b9627aad8ea8ed46","3721a416ed33435ebe994fcb5a64e3e1","c328b6b910b24285b23665ba8379751a","00e75dbcbcb0475ebaaf77aa532d919d","ac65e7aec00943a39d4dc9512b471a05","2cdb3706fa14468d933e16a14612e369"]},"executionInfo":{"status":"ok","timestamp":1617315441164,"user_tz":240,"elapsed":6358,"user":{"displayName":"APS360 GroupProj","photoUrl":"","userId":"01981303678726418317"}},"outputId":"0f730fd8-cec9-4a6f-d6dc-276c322da435"},"source":["resnet50 =torchvision.models.resnet50(pretrained=True)\n","n_inputs = resnet50.fc.in_features\n","\n","for param in resnet50.parameters():\n","  param.requires_grad = False"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7cce0a65b41f4777ba60a2d1eb5af882","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jr9nnWwBl05w","executionInfo":{"status":"ok","timestamp":1617315441193,"user_tz":240,"elapsed":6380,"user":{"displayName":"APS360 GroupProj","photoUrl":"","userId":"01981303678726418317"}}},"source":["resnet50.fc = nn.Sequential(\n","                      nn.Linear(n_inputs , 1024),\n","                      nn.BatchNorm1d(1024),\n","                      nn.Dropout(0.2),\n","                      nn.Linear(1024 , 512),\n","                      nn.Linear(512 , 4))"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v-3NA5UfmFXW"},"source":["### Confusion Matrix Plotting"]},{"cell_type":"code","metadata":{"id":"KICfobsYlfoU","executionInfo":{"status":"ok","timestamp":1617315441198,"user_tz":240,"elapsed":6371,"user":{"displayName":"APS360 GroupProj","photoUrl":"","userId":"01981303678726418317"}}},"source":["def get_accuracy_mat(model, data_loader):\n","    correct = 0\n","    total = 0\n","    \n","    # List of all the labels\n","    predicted_labels = [] # List of predictions made by model\n","    true_labels = []      # List of labels for images seen by model\n","    \n","    for imgs, labels in data_loader:\n","         \n","        output = model(imgs)\n","        \n","        #select index with maximum prediction score\n","        pred = output.max(1, keepdim=True)[1]\n","        correct += pred.eq(labels.view_as(pred)).sum().item()\n","        total += imgs.shape[0]\n","\n","        # Adding the labels/predictions\n","        predicted_labels.extend(pred.tolist())  # Adds model prediction to list\n","        true_labels.extend(labels.tolist())     # Adds true label to list\n","\n","    # Creates the normalized confusion matrix (remove normalize='true' flag for unnormalized) \n","    matrix = pd.DataFrame(confusion_matrix(true_labels,predicted_labels,normalize=None),['COVID-19', 'Normal', 'Pneumonial-Bacterial','Pneumonial-Viral'],\n","                          ['COVID-19', 'Normal', 'Pneumonial-Bacterial','Pneumonial-Viral'])\n","\n","    # Visual representation of matrix\n","    seaborn.heatmap(matrix, annot=True, fmt='g', cbar=False, cmap=\"YlGnBu\")\n","    \n","    # Returns accuracy \n","    return correct / total"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"DYC3XgULl3nM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617315441926,"user_tz":240,"elapsed":7087,"user":{"displayName":"APS360 GroupProj","photoUrl":"","userId":"01981303678726418317"}},"outputId":"de828757-2587-4655-d2e8-6ad061decd0d"},"source":["bs = 256\n","train_loader, val_loader, test_loader = load_aug_data(batch_size=bs)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Training data: 3080\n","Training Augmented data: 4616\n","Validation data: 1020\n","Testing data: 1024\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_jIMw8tmmVbE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617315455384,"user_tz":240,"elapsed":20529,"user":{"displayName":"APS360 GroupProj","photoUrl":"","userId":"01981303678726418317"}},"outputId":"790093e6-a8e1-498e-d009-75bb6d4fce25"},"source":["resnet18_model = torchvision.models.resnet18(pretrained=True)\n","n_inputs = resnet18_model.fc.in_features\n","\n","# for param in resnet50_model.parameters():\n","#   param.requires_grad = False\n","\n","resnet18_model.fc = nn.Sequential(\n","                      nn.Linear(n_inputs , 256),\n","                      nn.BatchNorm1d(256),\n","                      nn.Dropout(0.2),\n","                      nn.Linear(256 , 128),\n","                      nn.Linear(128 , 4))\n","state = torch.load('/content/gdrive/MyDrive/Training_Results/trial_1/model_trial_1/model_resnet18_bs256_lr0.0001_epoch2')\n","resnet18_model.load_state_dict(state)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"rV4OcCBgnYz0"},"source":["acc = get_accuracy_mat(resnet18_model, test_loader)\n","print(\"Test Accuracy of resnet18:\", acc)"],"execution_count":null,"outputs":[]}]}