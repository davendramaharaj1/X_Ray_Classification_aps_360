{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wd5LhLacoi0G"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.2 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "bda6d754138e49b2ebf8b651a77a14698e425fe7435331076ed05b1488b86230"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-MnjtoY_Xsc"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models\n",
        "from PIL import Image"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FemEIn0OHRCv",
        "outputId": "49ceeb2a-464f-46fb-82fe-e74f21a5c574"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd5LhLacoi0G"
      },
      "source": [
        "### Splitting Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVzjalQgklJe",
        "outputId": "5c5db3b5-b5d2-413c-a359-cd8e8269f1f9"
      },
      "source": [
        "# # Directory to get data from\n",
        "# dir = r'/content/gdrive/MyDrive/APS360/Project/SampleDataLarge/Pneumonia-Bacterial'\n",
        "# # Directory to store training set\n",
        "# train_dir = r'/content/gdrive/MyDrive/APS360/Project/sample_large/bacterial_train'\n",
        "# # Directory to store testing set\n",
        "# test_dir = r'/content/gdrive/MyDrive/APS360/Project/sample_large/bacterial_val'\n",
        "# # Directory to store validation path\n",
        "# valid_dir = r'/content/gdrive/MyDrive/APS360/Project/sample_large/bacterial_test'\n",
        "\n",
        "# # Get the list of file paths in the directory of dataset\n",
        "# files = [file for file in os.listdir(\n",
        "#     dir) if os.path.isfile(os.path.join(dir, file))]\n",
        "# # Input size of training set\n",
        "# train_count = np.round(70 / 100 * len(files))\n",
        "# # Input size of testing set\n",
        "# test_count = np.round(15 / 100 * len(files))\n",
        "# # Input size of validation set\n",
        "# valid_count = np.round(15 / 100 * len(files))\n",
        "# # Generate random numbers of file indices\n",
        "# random_indices = list(random.sample(range(0, len(files)), len(files)))\n",
        "# print(\"len(files)\", len(files))\n",
        "\n",
        "# # train_files indices\n",
        "# print(random_indices)\n",
        "\n",
        "# # training files\n",
        "# train_file_index = random_indices[0:int(train_count) + 1]\n",
        "# train_file_name = [files[i] for i in train_file_index]\n",
        "\n",
        "# # testing files\n",
        "# test_file_index = random_indices[int(\n",
        "#     train_count) + 1:int(train_count + test_count) + 1]\n",
        "# test_file_name = [files[i] for i in test_file_index]\n",
        "\n",
        "# # validation files\n",
        "# valid_file_index = random_indices[int(train_count + test_count) + 1:]\n",
        "# valid_file_name = [files[i] for i in valid_file_index]\n",
        "\n",
        "# # training files\n",
        "# for train in train_file_name:\n",
        "#     file = train\n",
        "#     shutil.copyfile(os.path.join(dir, file), os.path.join(train_dir, file))\n",
        "# # test_files\n",
        "# for test in test_file_name:\n",
        "#     file = test\n",
        "#     shutil.copyfile(os.path.join(dir, file), os.path.join(test_dir, file))\n",
        "\n",
        "# # valid_files\n",
        "# for valid in valid_file_name:\n",
        "#     file = valid\n",
        "#     shutil.copyfile(os.path.join(dir, file), os.path.join(valid_dir, file))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(files) 243\n",
            "[96, 76, 10, 46, 75, 26, 53, 238, 94, 206, 117, 81, 211, 176, 200, 186, 168, 216, 63, 142, 101, 103, 167, 99, 40, 27, 161, 25, 159, 172, 228, 213, 150, 37, 4, 52, 219, 199, 160, 74, 118, 184, 58, 11, 43, 174, 192, 189, 42, 116, 209, 197, 92, 31, 239, 44, 89, 1, 203, 180, 15, 88, 131, 132, 221, 242, 173, 32, 90, 98, 64, 19, 170, 77, 236, 62, 3, 124, 163, 109, 217, 154, 12, 232, 218, 104, 57, 187, 119, 133, 0, 229, 107, 128, 195, 204, 21, 22, 47, 240, 106, 16, 149, 157, 141, 115, 130, 45, 226, 34, 48, 177, 112, 143, 193, 227, 191, 233, 205, 135, 113, 35, 123, 136, 6, 224, 114, 198, 17, 8, 87, 29, 139, 155, 166, 153, 208, 196, 223, 84, 231, 138, 14, 148, 181, 151, 59, 127, 108, 65, 202, 18, 60, 2, 183, 51, 9, 28, 105, 85, 190, 49, 220, 121, 212, 169, 80, 23, 72, 54, 13, 68, 95, 61, 20, 179, 165, 129, 5, 70, 162, 102, 137, 56, 235, 97, 110, 156, 41, 125, 207, 111, 50, 214, 144, 67, 178, 120, 222, 134, 171, 71, 215, 140, 175, 185, 158, 126, 230, 24, 182, 237, 39, 201, 7, 241, 146, 33, 66, 69, 122, 83, 164, 86, 152, 55, 145, 91, 30, 100, 93, 36, 225, 82, 78, 79, 147, 188, 73, 210, 234, 38, 194]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKVpc5iUk5YV",
        "outputId": "66b22e1f-4767-482a-c17e-80f8d7231f9a"
      },
      "source": [
        "# # Directory to get data from\r\n",
        "# dir = r'/content/gdrive/MyDrive/APS360/Project/SampleDataLarge/Pneumonia-Viral'\r\n",
        "# # Directory to store training set\r\n",
        "# train_dir = r'/content/gdrive/MyDrive/APS360/Project/sample_large/viral_train'\r\n",
        "# # Directory to store testing set\r\n",
        "# test_dir = r'/content/gdrive/MyDrive/APS360/Project/sample_large/viral_val'\r\n",
        "# # Directory to store validation path\r\n",
        "# valid_dir = r'/content/gdrive/MyDrive/APS360/Project/sample_large/viral_test'\r\n",
        "\r\n",
        "# # Get the list of file paths in the directory of dataset\r\n",
        "# files = [file for file in os.listdir(\r\n",
        "#     dir) if os.path.isfile(os.path.join(dir, file))]\r\n",
        "# # Input size of training set\r\n",
        "# train_count = np.round(70 / 100 * len(files))\r\n",
        "# # Input size of testing set\r\n",
        "# test_count = np.round(15 / 100 * len(files))\r\n",
        "# # Input size of validation set\r\n",
        "# valid_count = np.round(15 / 100 * len(files))\r\n",
        "# # Generate random numbers of file indices\r\n",
        "# random_indices = list(random.sample(range(0, len(files)), len(files)))\r\n",
        "# print(\"len(files)\", len(files))\r\n",
        "\r\n",
        "# # train_files indices\r\n",
        "# print(random_indices)\r\n",
        "\r\n",
        "# # training files\r\n",
        "# train_file_index = random_indices[0:int(train_count) + 1]\r\n",
        "# train_file_name = [files[i] for i in train_file_index]\r\n",
        "\r\n",
        "# # testing files\r\n",
        "# test_file_index = random_indices[int(\r\n",
        "#     train_count) + 1:int(train_count + test_count) + 1]\r\n",
        "# test_file_name = [files[i] for i in test_file_index]\r\n",
        "\r\n",
        "# # validation files\r\n",
        "# valid_file_index = random_indices[int(train_count + test_count) + 1:]\r\n",
        "# valid_file_name = [files[i] for i in valid_file_index]\r\n",
        "\r\n",
        "# # training files\r\n",
        "# for train in train_file_name:\r\n",
        "#     file = train\r\n",
        "#     shutil.copyfile(os.path.join(dir, file), os.path.join(train_dir, file))\r\n",
        "# # test_files\r\n",
        "# for test in test_file_name:\r\n",
        "#     file = test\r\n",
        "#     shutil.copyfile(os.path.join(dir, file), os.path.join(test_dir, file))\r\n",
        "\r\n",
        "# # valid_files\r\n",
        "# for valid in valid_file_name:\r\n",
        "#     file = valid\r\n",
        "#     shutil.copyfile(os.path.join(dir, file), os.path.join(valid_dir, file))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(files) 250\n",
            "[42, 132, 138, 171, 170, 16, 175, 152, 20, 43, 213, 176, 126, 75, 28, 87, 244, 82, 74, 109, 6, 13, 84, 52, 195, 121, 66, 118, 24, 44, 49, 232, 163, 125, 209, 14, 113, 179, 154, 124, 181, 162, 218, 95, 133, 23, 80, 127, 139, 215, 115, 99, 180, 190, 226, 246, 10, 22, 204, 249, 92, 140, 210, 131, 187, 238, 70, 212, 120, 160, 21, 104, 145, 27, 8, 60, 107, 116, 17, 228, 184, 2, 166, 3, 4, 67, 167, 111, 108, 151, 169, 100, 182, 56, 142, 36, 240, 85, 158, 219, 174, 134, 178, 102, 137, 50, 155, 235, 53, 199, 206, 248, 71, 191, 19, 62, 90, 224, 98, 149, 18, 159, 48, 221, 1, 164, 207, 51, 129, 32, 157, 97, 222, 40, 231, 29, 135, 88, 96, 130, 25, 63, 237, 77, 69, 117, 91, 31, 208, 94, 173, 86, 46, 153, 185, 64, 177, 61, 101, 205, 217, 106, 26, 200, 89, 150, 223, 93, 147, 202, 68, 114, 79, 9, 197, 168, 58, 35, 188, 54, 59, 33, 112, 15, 196, 78, 30, 0, 216, 220, 34, 45, 156, 211, 57, 203, 198, 243, 229, 136, 39, 83, 245, 72, 242, 201, 193, 234, 105, 141, 41, 76, 236, 110, 119, 227, 183, 122, 239, 225, 12, 148, 214, 38, 144, 37, 241, 73, 47, 103, 81, 123, 165, 192, 146, 65, 11, 5, 143, 161, 7, 194, 128, 186, 55, 172, 233, 230, 247, 189]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSQx1glnlEjr",
        "outputId": "8901e7d5-cdb8-4be3-e837-2f6c30eb18ab"
      },
      "source": [
        "# # Directory to get data from\r\n",
        "# dir = r'/content/gdrive/MyDrive/APS360/Project/SampleDataLarge/Normal'\r\n",
        "# # Directory to store training set\r\n",
        "# train_dir = r'/content/gdrive/MyDrive/APS360/Project/sample_large/normal_train'\r\n",
        "# # Directory to store testing set\r\n",
        "# test_dir = r'/content/gdrive/MyDrive/APS360/Project/sample_large/normal_val'\r\n",
        "# # Directory to store validation path\r\n",
        "# valid_dir = r'/content/gdrive/MyDrive/APS360/Project/sample_large/normal_test'\r\n",
        "\r\n",
        "# # Get the list of file paths in the directory of dataset\r\n",
        "# files = [file for file in os.listdir(\r\n",
        "#     dir) if os.path.isfile(os.path.join(dir, file))]\r\n",
        "# # Input size of training set\r\n",
        "# train_count = np.round(70 / 100 * len(files))\r\n",
        "# # Input size of testing set\r\n",
        "# test_count = np.round(15 / 100 * len(files))\r\n",
        "# # Input size of validation set\r\n",
        "# valid_count = np.round(15 / 100 * len(files))\r\n",
        "# # Generate random numbers of file indices\r\n",
        "# random_indices = list(random.sample(range(0, len(files)), len(files)))\r\n",
        "# print(\"len(files)\", len(files))\r\n",
        "\r\n",
        "# # train_files indices\r\n",
        "# print(random_indices)\r\n",
        "\r\n",
        "# # training files\r\n",
        "# train_file_index = random_indices[0:int(train_count) + 1]\r\n",
        "# train_file_name = [files[i] for i in train_file_index]\r\n",
        "\r\n",
        "# # testing files\r\n",
        "# test_file_index = random_indices[int(\r\n",
        "#     train_count) + 1:int(train_count + test_count) + 1]\r\n",
        "# test_file_name = [files[i] for i in test_file_index]\r\n",
        "\r\n",
        "# # validation files\r\n",
        "# valid_file_index = random_indices[int(train_count + test_count) + 1:]\r\n",
        "# valid_file_name = [files[i] for i in valid_file_index]\r\n",
        "\r\n",
        "# # training files\r\n",
        "# for train in train_file_name:\r\n",
        "#     file = train\r\n",
        "#     shutil.copyfile(os.path.join(dir, file), os.path.join(train_dir, file))\r\n",
        "# # test_files\r\n",
        "# for test in test_file_name:\r\n",
        "#     file = test\r\n",
        "#     shutil.copyfile(os.path.join(dir, file), os.path.join(test_dir, file))\r\n",
        "\r\n",
        "# # valid_files\r\n",
        "# for valid in valid_file_name:\r\n",
        "#     file = valid\r\n",
        "#     shutil.copyfile(os.path.join(dir, file), os.path.join(valid_dir, file))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(files) 250\n",
            "[128, 152, 2, 12, 42, 140, 132, 20, 1, 144, 68, 60, 16, 193, 92, 181, 18, 214, 45, 73, 227, 163, 147, 63, 151, 15, 155, 180, 179, 217, 156, 82, 65, 98, 234, 113, 50, 88, 187, 27, 7, 171, 78, 85, 173, 10, 249, 200, 197, 149, 32, 51, 35, 202, 116, 17, 19, 136, 106, 192, 13, 108, 76, 235, 125, 239, 114, 138, 216, 24, 101, 33, 130, 178, 203, 224, 182, 122, 56, 90, 231, 243, 96, 11, 150, 31, 134, 199, 39, 49, 23, 89, 186, 145, 184, 204, 206, 75, 64, 242, 226, 124, 146, 97, 109, 0, 81, 232, 104, 111, 240, 28, 133, 119, 3, 209, 222, 158, 93, 176, 4, 102, 165, 205, 143, 169, 210, 148, 67, 170, 103, 168, 74, 34, 54, 208, 123, 99, 194, 212, 153, 107, 183, 154, 166, 225, 95, 37, 5, 38, 59, 8, 131, 118, 218, 69, 112, 228, 233, 221, 71, 139, 159, 9, 246, 175, 230, 247, 237, 48, 244, 58, 53, 29, 185, 117, 94, 57, 66, 91, 161, 160, 195, 21, 207, 44, 241, 36, 83, 25, 110, 172, 86, 41, 62, 72, 22, 84, 135, 248, 236, 238, 191, 196, 30, 115, 105, 52, 201, 198, 14, 79, 220, 190, 6, 229, 223, 157, 55, 87, 167, 162, 40, 46, 137, 245, 126, 219, 26, 189, 47, 127, 164, 215, 43, 121, 142, 70, 213, 77, 61, 141, 129, 80, 177, 100, 188, 120, 174, 211]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(batch_size=64, num_workers=1, small=False):\n",
        "  # Compose allows us to have multiple transformations to occur, where we first ensure our images are 224x224, and then\n",
        "  # convert the images to a tensor in the form (torch.Tensor([3, 224, 224])\n",
        "  transform_it = transforms.Compose([transforms.CenterCrop(224), transforms.ToTensor()])\n",
        "\n",
        "  # Check to see if we want to load the small dataset for over fitting\n",
        "  if small:\n",
        "    small_path = main_dir + '/Lab3a'\n",
        "    small_data = torchvision.datasets.ImageFolder(small_path, transform=transform_it)\n",
        "    return small_data\n",
        "\n",
        "  # Save the paths of each of the different types of data that are located in my drive \n",
        "  train_path = main_dir + '/Gesture_Dataset_Sorted/train_data'\n",
        "  val_path = main_dir + '/Gesture_Dataset_Sorted/val_data'\n",
        "  test_path = main_dir + '/Gesture_Dataset_Sorted/test_data'\n",
        "\n",
        "  # Load all of the data from my google drive\n",
        "  train_data = torchvision.datasets.ImageFolder(train_path, transform=transform_it)\n",
        "  val_data = torchvision.datasets.ImageFolder(val_path, transform=transform_it)\n",
        "  test_data = torchvision.datasets.ImageFolder(test_path, transform=transform_it)\n",
        "\n",
        "  return train_data, val_data, test_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_rxd7xpox9T"
      },
      "source": [
        "### CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypjup4QRfbFh"
      },
      "source": [
        " #Convolutional Neural Network Architecture for classifying chest xray images\n",
        "class Xray_Classifier(nn.Module):\n",
        "      def __init__(self):\n",
        "          self.name = \"Xray_Classifier\"\n",
        "          super(Xray_Classifier, self).__init__()\n",
        "          self.conv1 = nn.Conv2d(3, 5, 5) #in_channel=3, out_channel=5, kernel_size=5\n",
        "          self.pool = nn.MaxPool2d(2, 2) #kernel_size=2, stride=2 \n",
        "          self.conv2 = nn.Conv2d(5, 10, 5) #in_channel=5, out_channel=10, kernel_size=5\n",
        "          self.fc1 = nn.Linear(10*53*53, 30) #in_features=10*53*53, out_features=30\n",
        "          self.fc2 = nn.Linear(30, 4) #in_features=30, out_features=4\n",
        "\n",
        "      def forward(self, x):\n",
        "          x = self.pool(F.relu(self.conv1(x))) #apply pooling to 1st convolution layer\n",
        "          x = self.pool(F.relu(self.conv2(x))) #apply pooling to 2nd convolution layer\n",
        "          x = x.view(-1, 10*53*53)\n",
        "          x = F.relu(self.fc1(x))\n",
        "          x = self.fc2(x)\n",
        "          return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For the model checkpoints \n",
        "def get_model_name(name, batch_size, learning_rate, epoch):\n",
        "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object containing the hyperparameters\n",
        "    Returns:\n",
        "        path: A string with the hyperparameter name and value concatenated\n",
        "    \"\"\"\n",
        "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
        "                                                   batch_size,\n",
        "                                                   learning_rate,\n",
        "                                                   epoch)\n",
        "    return path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQImtzSgfbFi"
      },
      "source": [
        "def get_accuracy(model, data_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in data_loader:\n",
        "         \n",
        "        #############################################\n",
        "        #To Enable GPU Usage\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "          imgs = imgs.cuda()\n",
        "          labels = labels.cuda()\n",
        "        #############################################\n",
        "        \n",
        "        output = model(imgs)\n",
        "        \n",
        "        #select index with maximum prediction score\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += imgs.shape[0]\n",
        "    return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g23OgnifbFi"
      },
      "source": [
        "def train(model, batch_size=64, learning_rate=0.001, num_epochs=20):\n",
        "    torch.manual_seed(1000)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    iters, losses, train_acc, val_acc = [], [], [], []\n",
        "\n",
        "    # training\n",
        "    n = 0 # the number of iterations\n",
        "    for epoch in range(num_epochs):\n",
        "        for imgs, labels in iter(train_loader):\n",
        "          \n",
        "          \n",
        "            #############################################\n",
        "            #To Enable GPU Usage\n",
        "            if use_cuda and torch.cuda.is_available():\n",
        "              #print(\"GPU is Available\")\n",
        "              imgs = imgs.cuda()\n",
        "              labels = labels.cuda()\n",
        "            #############################################\n",
        "            \n",
        "              \n",
        "            out = model(imgs)             # forward pass\n",
        "            loss = criterion(out, labels) # compute the total loss\n",
        "            loss.backward()               # backward pass (compute parameter updates)\n",
        "            optimizer.step()              # make the updates for each parameter\n",
        "            optimizer.zero_grad()         # a clean up step for PyTorch\n",
        "\n",
        "            # save the current training information\n",
        "            iters.append(n)\n",
        "            losses.append(float(loss)/batch_size)             # compute *average* loss\n",
        "            train_acc.append(get_accuracy(model, train_loader)) # compute training accuracy \n",
        "            val_acc.append(get_accuracy(model, val_loader))  # compute validation accuracy\n",
        "            n += 1\n",
        "\n",
        "        # Print the accuracies of validation and training for each epoch to observe how it changes over time \n",
        "        print(\"epoch number: \", epoch+1, \"Training accuracy: \",train_acc[epoch], \"Validation accuracy: \", val_acc[epoch])\n",
        "        # Save the current model (checkpoint) to a file\n",
        "        model_path = get_model_name(model.name, batch_size, learning_rate, epoch)\n",
        "        torch.save(model.state_dict(), model_path)      \n",
        "\n",
        "    # plotting\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, train_acc, label=\"Train\")\n",
        "    plt.plot(iters, val_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Training Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use GPU\n",
        "use_cuda = True\n",
        "model_Xray= Xray_Classifier()\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_Xray.cuda()\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "else:\n",
        "  print('CUDA is not available.  Training on CPU ...')\n",
        "\n",
        "train(model_Xray, batch_size=64, learning_rate=0.001, num_epochs=15)"
      ]
    }
  ]
}